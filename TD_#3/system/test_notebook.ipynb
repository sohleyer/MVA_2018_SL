{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech and natural language processing - TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "from src.clean_utils import is_number, clean_tweet\n",
    "from src.levenshtein_distance import levenshtein_distance\n",
    "from context2vec.eval.explore_context2vec_func import explore_context2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_raw = 0\n",
    "last_raw = 10\n",
    "n_result = 1000  # number of search result to show\n",
    "f = open('CorpusBataclan_en.1M.raw.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge raws with corresponding tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_list = list(f)\n",
    "rtweet_list = []\n",
    "for raw in raw_list:\n",
    "    if raw[:2] == 'RT':\n",
    "        rtweet_list.append(raw)\n",
    "    else:\n",
    "        rtweet_list[-1] = rtweet_list[-1] + raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raws: 1000000\n",
      "Number of effective retweets: 598663\n"
     ]
    }
   ],
   "source": [
    "print 'Number of raws:', len(raw_list)\n",
    "print 'Number of effective retweets:', len(rtweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 4345.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean tweets:  0.0323951244354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tweets_to_clean = list(rtweet_list)\n",
    "rtweet_list = []\n",
    "for tweet in tqdm(tweets_to_clean[first_raw:last_raw]):\n",
    "    \n",
    "    tweet_cleaned = clean_tweet(tweet)\n",
    "    \n",
    "    # Add to the list\n",
    "    rtweet_list.append(tweet_cleaned)\n",
    "    \n",
    "stop = time.time()\n",
    "print 'Time to clean tweets: ', stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx,tweet in enumerate(rtweet_list[:100]):\n",
    "#    print str(idx)+'.', tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: ./MODEL_DIR/context2vec.ukwac.model.params\n",
      "Config:  {'config_path': './MODEL_DIR/', 'model_file': 'context2vec.ukwac.model', 'deep': 'yes', 'drop_ratio': '0.0', 'words_file': 'context2vec.ukwac.words.targets', 'unit': '300'}\n",
      "Time to import model context2vec: 66.8794682026\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model_param_file = './MODEL_DIR/context2vec.ukwac.model.params'\n",
    "model_reader = ModelReader(model_param_file)\n",
    "w = model_reader.w\n",
    "word2index = model_reader.word2index\n",
    "index2word = model_reader.index2word\n",
    "model = model_reader.model\n",
    "stop = time.time()\n",
    "print \"Time to import model context2vec:\", stop-start\n",
    "\n",
    "target_exp = re.compile('\\[.*\\]')\n",
    "# setup tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 it's disappointing how people nowadays think terrorism is linked to a religion.. \n",
      "it's disappointing how people nowadays think terrorism is linked to a religion..  \n",
      "\n",
      "1 israel killing muslims everyday and no one bats an eye. terrorist attack and muslims got the blame? how shallow can you be?   that bitch stops a show because someone spilled water on stage but puts on a show when terrorists attacki \n",
      ">> uncorrect word: attacki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> correction: tricia\n",
      ">> Levenshtein distance: 7.0\n",
      "israel killing muslims everyday and no one bats an eye. terrorist attack and muslims got the blame? how shallow can you be?   that bitch stops a show because someone spilled water on stage but puts on a show when terrorists tricia  \n",
      "\n",
      "2 french president francois hollande condemns the \"terrorist attacks of unprecedented proportions.\" pray for paris  \n",
      ">> uncorrect word: hollande\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 13.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> correction: <BOS>\n",
      ">> Levenshtein distance: 8.0\n",
      "french president francois hollande condemns the \"terrorist attacks of unprecedented proportions.\" pray for paris   \n",
      "\n",
      "3 ny lights in blue, white &amp; red as we stand in solidarity with the people of france:   \n",
      "ny lights in blue, white &amp; red as we stand in solidarity with the people of france:    \n",
      "\n",
      "4 my name is malik riaz. i am a muslim. i condemn the paris attack over 1.5 billion muslims do.   please remember this. good on you, paris!  facebook is asking people in paris to check in if they're safe  \n",
      "my name is malik riaz. i am a muslim. i condemn the paris attack over 1.5 billion muslims do.   please remember this. good on you, paris!  facebook is asking people in paris to check in if they're safe   \n",
      "\n",
      "5 prayers to the victims and survivors of the paris tragedy. we must not let the hate of a few come between us as humans on \n",
      "prayers to the victims and survivors of the paris tragedy. we must not let the hate of a few come between us as humans on  \n",
      "\n",
      "6 omni hotel in dallas is displaying french flag in solidarity with france.  \n",
      "omni hotel in dallas is displaying french flag in solidarity with france.   \n",
      "\n",
      "7 kkk murders ppl white people: ok but not all white people isis murders ppl white people: all muslims are terrorists. ba what the fuck is wrong with you, can you please learn how  to respect!!!  this hurts to look at, the ignorance and disrespect people still have disgusts me   \n",
      "kkk murders ppl white people: ok but not all white people isis murders ppl white people: all muslims are terrorists. ba what the fuck is wrong with you, can you please learn how  to respect!!!  this hurts to look at, the ignorance and disrespect people still have disgusts me    \n",
      "\n",
      "8 my heart is with those in paris.  new york stands with paris in 'solemn solidarity'  \n",
      "my heart is with those in paris.  new york stands with paris in 'solemn solidarity'   \n",
      "\n",
      "9 paris txi drivers turned off their meters, took people home for free - reports   \n",
      ">> uncorrect word: txi\n",
      ">> correction: ps.\n",
      ">> Levenshtein distance: 3.0\n",
      "paris txi drivers turned off their meters, took people home for free - reports    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalised_rtweet_list = []\n",
    "for idx,tweet in tqdm(enumerate(rtweet_list[first_raw:last_raw])):\n",
    "    print idx, tweet\n",
    "    tokenize_tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    for token in tokenize_tweet:\n",
    "        if is_number(token):\n",
    "            continue\n",
    "            \n",
    "        #find uncorrect words\n",
    "        if token not in word2index:  \n",
    "            print \">> uncorrect word:\", token\n",
    "            \n",
    "            #generate context\n",
    "            context = re.sub(token, '[]', tweet)\n",
    "            context_proposition = explore_context2vec(context, w, word2index, index2word, model, target_exp, n_result)\n",
    "            \n",
    "            #find clother word in context\n",
    "            min_dist = np.inf \n",
    "            for proposition in context_proposition:\n",
    "                dist = levenshtein_distance(token,proposition)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    correct_word = proposition[0]\n",
    "            print \">> correction:\", correct_word\n",
    "            print '>> Levenshtein distance:', min_dist\n",
    "            correct_tweet = re.sub(token, correct_word, tweet)\n",
    "        else:\n",
    "            correct_tweet = tweet\n",
    "    print correct_tweet, '\\n'\n",
    "    \n",
    "    normalised_rtweet_list.append(correct_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hollande 1.0 holland\n"
     ]
    }
   ],
   "source": [
    "uncorrect_words = ['hollande']\n",
    "for uncorrect_word in uncorrect_words:\n",
    "    min_dist = np.inf\n",
    "    for word_dict in word2index:\n",
    "        dist = levenshtein_distance(uncorrect_word, word_dict)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            correct_word = word_dict\n",
    "    print uncorrect_word, min_dist, correct_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TD_3]",
   "language": "python",
   "name": "conda-env-TD_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
